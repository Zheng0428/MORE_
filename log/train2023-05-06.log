WARNING:__main__:*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
Let's use 4 GPUs!
Let's use 4 GPUs!
Let's use 4 GPUs!
Let's use 4 GPUs!
Load 22109 data from split(s) train,nominival.
Load 22109 data from split(s) train,nominival.
Load 22109 data from split(s) train,nominival.
Load 22109 data from split(s) train,nominival.
Load 3142 data from split(s) valid.Load 3142 data from split(s) valid.Load 3142 data from split(s) valid.


Load 3142 data from split(s) valid.
BertAdam Total Iters: 5528
BertAdam Total Iters: 5528
BertAdam Total Iters: 5528
BertAdam Total Iters: 5528
tensor(20.1100, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(20.1100, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(20.1100, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(20.1100, device='cuda:3', grad_fn=<NllLossBackward>)
./src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
./src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
./src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
./src/lxrt/optimization.py:142: UserWarning: This overload of add_ is deprecated:
	add_(Number alpha, Tensor other)
Consider using one of the following signatures instead:
	add_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1025.)
  next_m.mul_(beta1).add_(1 - beta1, grad)
tensor(19.9712, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(19.9712, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(19.9712, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(19.9712, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(19.9196, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(19.9196, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(19.9196, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(19.9196, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(19.6098, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(19.6098, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(19.6098, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(19.6098, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(19.2972, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(19.2972, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(19.2972, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(19.2972, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(18.6803, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(18.6803, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(18.6803, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(18.6803, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(17.7085, device='cuda:0', grad_fn=<NllLossBackward>)
tensor(17.7085, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(17.7085, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(17.7085, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(16.8291, device='cuda:1', grad_fn=<NllLossBackward>)
tensor(16.8291, device='cuda:2', grad_fn=<NllLossBackward>)
tensor(16.8291, device='cuda:3', grad_fn=<NllLossBackward>)
tensor(16.8291, device='cuda:0', grad_fn=<NllLossBackward>)
Traceback (most recent call last):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 640583) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/biao/MORE_/src/tasks/more.py", line 234, in <module>
    more.train(
  File "/home/biao/MORE_/src/tasks/more.py", line 122, in train
    for idx, (lxmert_out, rtg, traj_mask, timesteps) in enumerate(loader):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 640583) exited unexpectedly
Traceback (most recent call last):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 640679) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/biao/MORE_/src/tasks/more.py", line 234, in <module>
    more.train(
  File "/home/biao/MORE_/src/tasks/more.py", line 122, in train
    for idx, (lxmert_out, rtg, traj_mask, timesteps) in enumerate(loader):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 640679) exited unexpectedly
Traceback (most recent call last):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 990, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/queue.py", line 179, in get
    self.not_empty.wait(remaining)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/threading.py", line 306, in wait
    gotit = waiter.acquire(True, timeout)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 640680) is killed by signal: Killed. 

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/home/biao/MORE_/src/tasks/more.py", line 234, in <module>
    more.train(
  File "/home/biao/MORE_/src/tasks/more.py", line 122, in train
    for idx, (lxmert_out, rtg, traj_mask, timesteps) in enumerate(loader):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 521, in __next__
    data = self._next_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1186, in _next_data
    idx, data = self._get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1142, in _get_data
    success, data = self._try_get_data()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 1003, in _try_get_data
    raise RuntimeError('DataLoader worker (pid(s) {}) exited unexpectedly'.format(pids_str)) from e
RuntimeError: DataLoader worker (pid(s) 640680) exited unexpectedly
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 640521) of binary: /home/biao/miniconda3/envs/MORE/bin/python
/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py:367: UserWarning: 

**********************************************************************
               CHILD PROCESS FAILED WITH NO ERROR_FILE                
**********************************************************************
CHILD PROCESS FAILED WITH NO ERROR_FILE
Child process 640521 (local_rank 1) FAILED (exitcode 1)
Error msg: Process failed with exitcode 1
Without writing an error file to <N/A>.
While this DOES NOT affect the correctness of your application,
no trace information about the error will be available for inspection.
Consider decorating your top level entrypoint function with
torch.distributed.elastic.multiprocessing.errors.record. Example:

  from torch.distributed.elastic.multiprocessing.errors import record

  @record
  def trainer_main(args):
     # do train
**********************************************************************
  warnings.warn(_no_error_file_warning_msg(rank, failure))
Traceback (most recent call last):
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/runpy.py", line 192, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/runpy.py", line 85, in _run_code
    exec(code, run_globals)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/run.py", line 702, in <module>
    main()
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 361, in wrapper
    return f(*args, **kwargs)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/run.py", line 698, in main
    run(args)
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/run.py", line 689, in run
    elastic_launch(
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 116, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/biao/miniconda3/envs/MORE/lib/python3.8/site-packages/torch/distributed/launcher/api.py", line 244, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
*********************************************
  /home/biao/MORE_/src/tasks/more.py FAILED  
=============================================
Root Cause:
[0]:
  time: 2023-05-06_19:14:46
  rank: 1 (local_rank: 1)
  exitcode: 1 (pid: 640521)
  error_file: <N/A>
  msg: "Process failed with exitcode 1"
=============================================
Other Failures:
  <NO_OTHER_FAILURES>
*********************************************

